# Low-Latency-Prompt-Optimization-with-Structured-Complementary-Prompt

[![EACL 2026](https://img.shields.io/badge/EACL-2026-blue)](https://2026.eacl.org/)
[![Paper](https://img.shields.io/badge/Paper-PDF-red)]()
[![License](https://img.shields.io/badge/License-MIT-green)]()

This repository contains the official implementation of the paper "Donâ€™t Generate, Classify! Low-Latency Prompt Optimization with Structured Complementary Prompt" published at **EACL 2026** (19th Conference of the European Chapter of the Association for Computational Linguistics).

## ğŸ“‹ Overview

This project implements LLPO (Low Latency Prompt Optimization), a novel approach that uses classification-based methods instead of generation-based approaches to optimize prompts for language models. By using a lightweight classifier to predict the appropriate task/domain category and selecting a pre-optimized structured prompt, LLPO significantly reduces inference latency while maintaining high response quality.

The main components include:

- **Field Clustering**: Semantically similar domain and prompt attributes are grouped to construct a compact label space for classification.
- **Prompt Optimization**: A pretrained encoder model rapidly predicts structured prompt fields from the user instruction and populates them into a predefined system-prompt template, enabling fast and consistent optimization without autoregressive generation.

Compared with existing generation-based optimization methods, LLPO delivers:

âœ… Comparable or improved response quality

âœ… Up to ~2000Ã— lower optimization latency

âœ… Stable and reproducible system-prompt structure

LLPO is ideal for real-time or latency-sensitive LLM applications, where prompt optimization overhead must remain minimal.

## ğŸ—ï¸ Project Structure

```
LLPO_github/
â”œâ”€â”€ clustering/                          # Field clustering module
â”‚   â”œâ”€â”€ fields_clustering_kmeans.py      # K-means clustering
â”‚   â”œâ”€â”€ fields_clustering_agglomerative.py  # Hierarchical clustering
â”‚   â”œâ”€â”€ functions.py                     # Common utility functions
â”‚   â””â”€â”€ sh/                              # Execution scripts
â”œâ”€â”€ classifier/                          # Classifier training module
â”‚   â”œâ”€â”€ train.py                         # Multi-task classifier training
â”‚   â””â”€â”€ sh/                              # Model-specific training scripts
â”‚       â”œâ”€â”€ RoBERTa_kmeans.sh
â”‚       â”œâ”€â”€ RoBERTa_agg.sh
â”‚       â”œâ”€â”€ DeBERTa_kmeans.sh
â”‚       â”œâ”€â”€ DeBERTa_agg.sh
â”‚       â”œâ”€â”€ ModernBERT_kmeans.sh
â”‚       â””â”€â”€ ModernBERT_agg.sh
â”œâ”€â”€ evaluation/                          # Evaluation module
â”‚   â”œâ”€â”€ optimization/                    # Optimization algorithms
â”‚   â”‚   â”œâ”€â”€ LLPO_optimization.py         # Low Latency Prompt Optimization
â”‚   â”‚   â”œâ”€â”€ BPO_optimization.py          # Batch Prompt Optimization
â”‚   â”‚   â”œâ”€â”€ PAS_optimization.py          # Prompt Augmentation Strategy
â”‚   â”‚   â””â”€â”€ FIPO_optimization.py         # Field-wise Instruction Prompt Optimization
â”‚   â”œâ”€â”€ inference/                       # Inference scripts
â”‚   â”‚   â”œâ”€â”€ inference_OpenAI.py
â”‚   â”‚   â”œâ”€â”€ inference_Anthropic.py
â”‚   â”‚   â””â”€â”€ inference_opensource.py
â”‚   â”œâ”€â”€ scoring/                         # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ scoring_gpt.py
â”‚   â”‚   â””â”€â”€ win_tie_lose.py
â”‚   â””â”€â”€ sh/                              # Execution scripts
â”‚       â”œâ”€â”€ optimization/
â”‚       â””â”€â”€ inference/
â”œâ”€â”€ prompts/                             # Prompt templates used for building the SCP dataset
â”‚
â””â”€â”€ datasets/                            # Datasets
    â”œâ”€â”€ clusters/                        # Clustering results
    â”‚   â”œâ”€â”€ final_kmeans/
    â”‚   â””â”€â”€ final_agglomerative/
    â””â”€â”€ SCP/                             # SCP dataset
        â”œâ”€â”€ SCP.json
        â””â”€â”€ minilm_field_embeddings.pkl               
```

## ğŸ› ï¸ Environment Setup

Install the required dependencies using pip:

```bash
pip install -r requirements.txt
```

## ğŸ“Š Datasets

You can download it here:
ğŸ‘‰ [Dataset](YOUR_LINK_HERE)

### SCP Dataset
- `SCP.json`: Main dataset with instruction examples
- `minilm_field_embeddings.pkl`: Field embeddings (using MiniLM model)
- `prompts/`: Prompt templates for different optimization methods
  - `BPO_prompt.txt`: Template for Batch Prompt Optimization
  - `PAS_prompt.txt`: Template for Prompt Augmentation Strategy

### Cluster Data
- `final_kmeans/`: K-means clustering results
- `final_agglomerative/`: Hierarchical clustering results

Each folder contains:
- `field_clusters_*.json`: Label groups by cluster
- `lookup_table_*.json`: Label conversion table
- `replaced_label_data_*.json`: Training data with clustered labels applied


## ğŸš€ Quick Start

### 1. Field Clustering

Cluster diverse domain labels into semantically similar groups.

#### K-means Clustering
```bash
python clustering/fields_clustering_kmeans.py
```

#### Hierarchical Clustering (Agglomerative)
```bash
python clustering/fields_clustering_agglomerative.py
```

**Output Files**:
- `field_clusters_{method}.json`: Clustering results
- `lookup_table_{method}.json`: Original label â†’ Representative label mapping
- `replaced_label_data_{method}.json`: Dataset with clustered labels

### 2. Classifier Training

Train multi-task classifiers using the clustered labels.

#### RoBERTa Model (K-means Clusters)
```bash
bash classifier/sh/RoBERTa_kmeans.sh
bash classifier/sh/RoBERTa_agg.sh
```

#### DeBERTa Model (Agglomerative Clusters)
```bash
bash classifier/sh/DeBERTa_kmeans.sh
bash classifier/sh/DeBERTa_agg.sh
```

#### ModernBERT Model
```bash
bash classifier/sh/ModernBERT_kmeans.sh
bash classifier/sh/ModernBERT_agg.sh
```

**Key Hyperparameters**:
- `--model_name`: Pre-trained model (RoBERTa, DeBERTa, ModernBERT, etc.)
- `--batch_size`: Batch size
- `--accumulation_steps`: Gradient accumulation steps
- `--learning_rate`: Learning rate
- `--dropout_rate`: Dropout rate
- `--gamma`: Gamma parameter for Focal Loss

### 3. Prompt Optimization and Inference

#### LLPO (Low Latency Prompt Optimization)
```bash
bash evaluation/sh/optimization/optimized_LLPO.sh
```

#### Other Prompt Optimization Methods
```bash
bash evaluation/sh/optimization/optimized_BPO.sh    # Batch Prompt Optimization
bash evaluation/sh/optimization/optimized_PAS.sh    # Prompt Augmentation Strategy
bash evaluation/sh/optimization/optimized_FIPO.sh   # Field-wise Instruction Prompt Optimization
```

#### Inference
```bash
bash evaluation/sh/inference/inference_LLPO.sh
bash evaluation/sh/inference/inference_BPO.sh
bash evaluation/sh/inference/inference_FIPO.sh
bash evaluation/sh/inference/inference_PAS.sh
```


## ğŸ“ˆ Evaluation

### GPT-based Scoring
```bash
python evaluation/scoring/scoring_gpt.py -a our_data.jsonl -b nonoutdata.jsonl -o output.jsonl
```

### Win/Tie/Lose Analysis
```bash
python evaluation/scoring/win_tie_lose.py
```

## ğŸ› ï¸ Supported Models

- **RoBERTa**: `FacebookAI/roberta-large`
- **DeBERTa**: `microsoft/deberta-v3-large`
- **ModernBERT**: `answerdotai/ModernBERT-large`
- Other HuggingFace Transformers compatible models

## ğŸ“ Citation

If you use this code, please cite:

```bibtex
@inproceedings{dont-generate-classify-2026,
  title={Don't Generate, Classify: Low Latency Prompt Optimization},
  author={[Authors]},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
  year={2026}
}
```
